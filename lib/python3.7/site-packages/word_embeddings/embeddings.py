import tensorflow as tf
import numpy as np
import pickle as pk
from pathlib import Path

INPUT_EMBEDDING_SIZE = 50
OUTPUT_EMBEDDING_SIZE = 100
OUTPUT_VOCAB_SIZE = 1150624

def load_input_embeddings():
    #GloVe embeddings for input sentences (English)
    embed_file = Path("embeddings.pkl")
    if embed_file.is_file():
        # embeddings already serialized, just load them
        print(f"Local Embeddings pickle found, loading...")
        with open("embeddings.pkl", 'rb') as f:
            return pk.load(f)
    else:
        # create the embeddings
        print(f"Building embeddings dictionary...")
        data = open('../lib/python3.7/site-packages/word_embeddings/glove.6B.50d.txt', 'r', encoding="utf-8")
        embeddings = [[0] * INPUT_EMBEDDING_SIZE]
        word_index_dict = {'UNK': 0}  # first row is for unknown words
        index = 1
        for line in data:
            split_line = line.split()
            word = tf.compat.as_str(split_line[0])
            embedding = [float(val) for val in split_line[1:]]
            embeddings.append(embedding)
            word_index_dict[word] = index
            index += 1
        data.close()

        # pickle them
        with open('embeddings.pkl', 'wb') as f:
            print(f"Creating local embeddings pickle for faster loading...")
            # Pickle the 'data' dictionary using the highest protocol available.
            pk.dump((embeddings, word_index_dict), f, pk.HIGHEST_PROTOCOL)

    return embeddings, word_index_dict


def glove_embeddings(embeddings, embedding_dict, in_sentence, embed_dim):
    num_words = len(in_sentence)
    embedded_sen = np.zeros([num_words, embed_dim])
    for w in range(num_words):
        embedded_sen[w, :] = embeddings[embedding_dict.get(in_sentence[w], 0)]
    return embedded_sen

def posit_encode(num_words, embed_dim):

    posit_encodings = np.zeros([num_words, embed_dim])
    for t in range(num_words):
        for i in range(embed_dim):
            k = int(i/2)
            wk = 1.0/(np.power(10000, (2*k/embed_dim)))
            if np.mod(i, 2) == 0:
                posit_encodings[t, i] = np.sin(wk*t)
            else:
                posit_encodings[t, i] = np.cos(wk*t)

    return posit_encodings